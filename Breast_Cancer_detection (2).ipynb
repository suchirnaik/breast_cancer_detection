{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3a3b5e",
   "metadata": {},
   "source": [
    "\n",
    "\t•\tazureml-sdk: Installs the Azure Machine Learning SDK, which is used for building and managing machine learning workflows on Azure. It includes tools for training models, managing datasets, deploying models, and more.\n",
    "\t•\tpillow: Installs the Pillow library, a Python Imaging Library (PIL) fork. This is typically used for image processing tasks, such as loading and manipulating image data.\n",
    "\t•\tmatplotlib: Installs Matplotlib, a plotting library used for creating static, interactive, and animated visualizations in Python. This can be helpful for data visualization in machine learning tasks.\n",
    "\t•\tazure-ai-ml: Installs the Azure AI Machine Learning SDK, which provides specific functionalities for working with machine learning services in Azure AI. It includes tools for managing datasets, running experiments, and managing ML workflows.\n",
    "\t•\tazure-identity: Installs the Azure Identity SDK, which helps authenticate to Azure services securely, often via managed identities or credentials such as Service Principal, OAuth, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1aff6a-25da-4e1d-83e9-95d0a68d0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azureml-sdk pillow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5efef-d681-409b-a504-3c663d94013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-ml\n",
    "!pip install azure-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c54819",
   "metadata": {},
   "source": [
    "\n",
    "\t•\tfrom azureml.core import Workspace: This imports the Workspace class from the Azure ML SDK. The Workspace object represents a centralized environment where you can store datasets, models, experiments, and compute targets.\n",
    "\t•\tws = Workspace(...): This creates a Workspace object by passing the following parameters:\n",
    "\t•\tsubscription_id: The Azure subscription ID where your resources (like the workspace) are located.\n",
    "\t•\tresource_group: The resource group under which the workspace is organized. Resource groups are containers for managing related Azure resources.\n",
    "\t•\tworkspace_name: The name of the specific Azure ML workspace you’re connecting to. In this case, it’s \"Breast_cancer_detection\".\n",
    "\t•\tprint(ws.name, ws.location, ws.resource_group): This line prints the workspace’s:\n",
    "\t•\tname: The name of the workspace (in this case, \"Breast_cancer_detection\").\n",
    "\t•\tlocation: The geographic location where the workspace is hosted (e.g., \"eastus2\").\n",
    "\t•\tresource_group: The resource group name (\"naiks01-rg\").\n",
    "\n",
    "This code effectively connects to your Azure Machine Learning workspace, allowing you to perform tasks like data management, model training, and deployment on Azure. It also confirms the connection by printing workspace details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83749a1b-933f-4353-aece-972ff1e5cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace(subscription_id=\"1eec3e0f-7d92-4d23-a1ec-35283850f6c3\",\n",
    "               resource_group=\"naiks01-rg\",\n",
    "               workspace_name=\"Breast_cancer_detection\")\n",
    "print(ws.name, ws.location, ws.resource_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d7eb4f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Importing Required Libraries:\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "\t•\tMLClient: This class is part of the azure-ai-ml SDK. It allows you to interact with Azure Machine Learning services and manage various resources like datasets, models, and experiments. It provides methods to work with resources stored in the Azure ML workspace.\n",
    "\t•\tDefaultAzureCredential: This class is from the azure-identity library. It provides a simplified way to authenticate to Azure services using a variety of methods, such as:\n",
    "\t•\tEnvironment variables\n",
    "\t•\tManaged identities (for Azure services)\n",
    "\t•\tAzure CLI credentials\n",
    "\t•\tVisual Studio Code credentials\n",
    "\t•\tAnd more\n",
    "\t•\tIt automatically selects the appropriate credential method based on the environment.\n",
    "\n",
    "2. Initializing the MLClient:\n",
    "\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(),\n",
    "    subscription_id=\"1eec3e0f-7d92-4d23-a1ec-35283850f6c3\",\n",
    "    resource_group_name=\"naiks01-rg\",\n",
    "    workspace_name=\"Breast_cancer_detection\"\n",
    ")\n",
    "\n",
    "\t•\tThis initializes the MLClient object, which is used to interact with Azure ML resources (such as datasets, models, experiments).\n",
    "\t•\tParameters:\n",
    "\t•\tDefaultAzureCredential(): Provides the authentication needed to access Azure resources. It will automatically select the appropriate authentication method.\n",
    "\t•\tsubscription_id: Specifies the Azure subscription ID where the Azure ML workspace is located.\n",
    "\t•\tresource_group_name: Specifies the resource group containing the workspace.\n",
    "\t•\tworkspace_name: Specifies the name of the Azure ML workspace.\n",
    "\n",
    "By initializing the MLClient, you’re able to access and manage your resources within the Azure ML workspace.\n",
    "\n",
    "3. Connecting to the Data Asset:\n",
    "\n",
    "data_asset = ml_client.data.get(name=\"Mammograms\", version=\"1\")\n",
    "\n",
    "\t•\tThis line retrieves a data asset from the Azure ML workspace using the ml_client.\n",
    "\t•\tml_client.data.get(): This method allows you to fetch a data asset from the workspace. You specify:\n",
    "\t•\tname=\"Mammograms\": The name of the data asset you want to access. In this case, the dataset is named “Mammograms,” which likely contains data related to breast cancer detection.\n",
    "\t•\tversion=\"1\": The version of the dataset you want to access. In this case, the first version of the “Mammograms” dataset is being retrieved. If you need a different version, you can change this number.\n",
    "\n",
    "4. Printing Data Asset Details:\n",
    "\n",
    "print(f\"Name: {data_asset.name}\")\n",
    "print(f\"Path: {data_asset.path}\")\n",
    "\n",
    "\t•\tAfter retrieving the data asset, these lines print out:\n",
    "\t•\tdata_asset.name: The name of the data asset (e.g., “Mammograms”).\n",
    "\t•\tdata_asset.path: The path to the data asset, which provides the location of the data in Azure storage or data lake. This is important because you will use this path to access the dataset for further processing or training.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\t•\tThis code connects to Azure ML, authenticates using the DefaultAzureCredential, and retrieves a dataset named “Mammograms” from the workspace.\n",
    "\t•\tIt prints out the name and path of the dataset to confirm the connection and provide details about the dataset location.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653c1e8-31c4-468c-a7d7-99211a1a1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# Initialize MLClient\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(),\n",
    "    subscription_id=\"1eec3e0f-7d92-4d23-a1ec-35283850f6c3\",\n",
    "    resource_group_name=\"naiks01-rg\",\n",
    "    workspace_name=\"Breast_cancer_detection\"\n",
    ")\n",
    "\n",
    "# Connect to the data asset\n",
    "data_asset = ml_client.data.get(name=\"Mammograms\", version=\"1\")  \n",
    "\n",
    "# Print data asset details\n",
    "print(f\"Name: {data_asset.name}\")\n",
    "print(f\"Path: {data_asset.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f46cc-8e1a-4e08-96bc-115e7910a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = data_asset.path\n",
    "print(f\"Data Path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02634d52",
   "metadata": {},
   "source": [
    "Environment Creation: A new environment named \"breast-cancer-env\" is created (or an existing one is updated).\n",
    "\t•\tPackage Addition: The joblib Python package is added to the environment, ensuring that it is available for any scripts or models using this environment.\n",
    "\t•\tEnvironment Registration: The environment is then registered in the Azure ML workspace, making it available for use in Azure ML experiments.\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1882c44-4b03-4b83-8947-dd9ebc8216c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "\n",
    "env = Environment(name=\"breast-cancer-env\")\n",
    "env.python.conda_dependencies.add_pip_package(\"joblib\")\n",
    "\n",
    "\n",
    "env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa0fed7",
   "metadata": {},
   "source": [
    "The lines of code you provided are doing the following:\n",
    "\n",
    "1. Joining Paths:\n",
    "\n",
    "benign_folder = os.path.join(data_path, \"Benign\")\n",
    "malignant_folder = os.path.join(data_path, \"Malignant\")\n",
    "\n",
    "\t•\tos.path.join(): This function is used to concatenate paths in a platform-independent way. It ensures that the correct directory separator (/ or \\, depending on the operating system) is used.\n",
    "\t•\tdata_path: This is the path to the directory or dataset location, which has been defined earlier in the code (likely as the path to the “Mammograms” dataset in your Azure ML workspace).\n",
    "\t•\t\"Benign\" and \"Malignant\": These are folder names likely corresponding to two categories of mammogram images—benign (non-cancerous) and malignant (cancerous). These folders will contain images or data files for each type of breast cancer case.\n",
    "\t•\tResult:\n",
    "\t•\tbenign_folder: This will store the full path to the “Benign” folder inside the dataset, which contains images of benign (non-cancerous) cases.\n",
    "\t•\tmalignant_folder: This will store the full path to the “Malignant” folder inside the dataset, which contains images of malignant (cancerous) cases.\n",
    "\n",
    "Summary:\n",
    "\n",
    "\t•\tThese lines are constructing full paths to two subdirectories—“Benign” and “Malignant”—inside the data_path. These directories likely hold the relevant datasets for training a breast cancer detection model, where benign and malignant refer to the two classes (types of tumors) the model will classify.\n",
    "\n",
    "Would you like to know how to use these directories further, or do you have any specific questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9413378-927d-4d55-8e31-b7837042d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_folder = os.path.join(data_path, \"Benign\")\n",
    "malignant_folder = os.path.join(data_path, \"Malignant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54dff39",
   "metadata": {},
   "source": [
    "In the code you provided, the Azure ML data asset folder (such as the “Mammograms” folder) serves a specific purpose in the Azure ML workflow, even though you’re directly accessing Azure Blob Storage for loading the images. Here’s why the Azure ML data asset is still useful, even if you’re directly interacting with Blob Storage for training:\n",
    "\n",
    "1. Seamless Integration with Azure ML Pipelines:\n",
    "\n",
    "\t•\tWhen you create a data asset in Azure ML, you’re essentially registering a reference to your data (in Blob Storage or Data Lake) within the Azure ML environment. This data asset is integrated into Azure ML’s infrastructure and makes it easier to use in experiments, pipelines, and model training without manually dealing with storage connections each time.\n",
    "\t•\tAzure ML Pipelines can directly use data assets, allowing you to refer to the data abstractly, making the entire workflow easier to manage.\n",
    "\n",
    "2. Versioning:\n",
    "\n",
    "\t•\tOne key benefit of using Azure ML Data Assets is versioning. When you register a data asset, Azure ML tracks different versions of the dataset. This is helpful when you want to:\n",
    "\t•\tKeep track of changes in the data (e.g., updates or new versions of the dataset).\n",
    "\t•\tReproduce experiments with the same version of the data, ensuring consistency in results.\n",
    "\t•\tWithout using the data asset, you’d have to manually track the version of the data you’re using (e.g., by naming your blobs or folders with version identifiers), which could become cumbersome.\n",
    "\n",
    "3. Centralized Data Management:\n",
    "\n",
    "\t•\tStoring data as a data asset in Azure ML centralizes your data management. The Azure ML workspace becomes the place where you organize, track, and access all datasets. You don’t have to manually reference paths and connections every time you work with the data in different experiments or pipelines.\n",
    "\t•\tThis is especially helpful when working in teams, as everyone can easily access the same dataset through Azure ML, reducing errors from working with mismatched paths or versions.\n",
    "\n",
    "4. Data Sharing and Collaboration:\n",
    "\n",
    "\t•\tIf you are working in a collaborative environment, data assets make it easier to share datasets with other team members or teams within Azure ML. When you register a dataset as a data asset, it becomes available for use by anyone with the appropriate access in the Azure ML workspace.\n",
    "\t•\tIf you didn’t use data assets and only referenced Blob Storage directly, data sharing would require sharing specific storage access keys, which is less flexible and can lead to security concerns.\n",
    "\n",
    "5. Security and Access Control:\n",
    "\n",
    "\t•\tAzure ML Data Assets allow you to manage access control and security more easily than direct access to Blob Storage. Azure ML integrates with Azure Active Directory (AAD) for role-based access control (RBAC), so you can restrict access to the data based on user roles, making it more secure and easier to manage in a multi-user environment.\n",
    "\n",
    "6. Simplified Workflow:\n",
    "\n",
    "\t•\tUsing data assets simplifies your workflow. Instead of manually handling the connection to Blob Storage (downloading blobs, managing credentials, paths, etc.), the data asset lets you treat the data as a managed resource within Azure ML.\n",
    "\t•\tThis is especially useful in automated machine learning pipelines, where you’d prefer not to deal with the complexities of file system management but rather refer to registered datasets in a uniform way.\n",
    "\n",
    "7. Best Practices in Azure ML:\n",
    "\n",
    "\t•\tAzure ML Data Assets are a best practice in Azure ML when working with datasets, as they provide a layer of abstraction that makes data management easier, more scalable, and more secure. Even if you’re directly accessing Blob Storage, it’s often preferable to register the dataset as a data asset to take full advantage of Azure ML’s features.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Even though you’re directly accessing Azure Blob Storage for loading the images, the Azure ML Data Asset serves as an organizational and management tool within Azure ML:\n",
    "\t•\tIt simplifies and standardizes data access.\n",
    "\t•\tIt supports versioning, security, and access control.\n",
    "\t•\tIt integrates seamlessly with Azure ML pipelines, experiments, and workflows.\n",
    "\n",
    "By creating the data asset in Azure ML, you’re aligning with best practices that make it easier to manage, share, and secure data across your machine learning projects.\n",
    "\n",
    "Let me know if you’d like to dive deeper into how Azure ML data assets are used in pipelines or experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc88b5-2bef-43eb-ab49-8c44ab35d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import ContainerClient\n",
    "from PIL import Image\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=datalakebreastcancer;AccountKey=70BHuecX7ho/jdOKwWKByc/iUg6lGK6RpvwY6A2dTJZGAkjCPxqz8hbBgjrU9VpZjTCCKHMCyZ9/+AStzKf5tQ==;EndpointSuffix=core.windows.net\"\n",
    "container_name = \"breastcancermammograms\"\n",
    "\n",
    "# Initialize the container client\n",
    "container_client = ContainerClient.from_connection_string(connection_string, container_name)\n",
    "\n",
    "# Function to load and preprocess images from a specific folder\n",
    "def load_images_from_azure(subfolder_path, label):\n",
    "    data = []\n",
    "    labels = []\n",
    "    blobs = container_client.list_blobs(name_starts_with=f\"{subfolder_path}/\")\n",
    "    for blob in blobs:\n",
    "        # Download the blob content\n",
    "        blob_data = container_client.download_blob(blob.name).readall()\n",
    "        # Open the image using PIL\n",
    "        image = Image.open(io.BytesIO(blob_data)).resize((224, 224)).convert(\"RGB\")\n",
    "        data.append(np.array(image))\n",
    "        labels.append(label)\n",
    "    return data, labels\n",
    "\n",
    "# Load benign and malignant images\n",
    "benign_data, benign_labels = load_images_from_azure(\"Mammograms/Benign\", 0)\n",
    "malignant_data, malignant_labels = load_images_from_azure(\"Mammograms/Malignant\", 1)\n",
    "\n",
    "# Combine the data\n",
    "data = np.array(benign_data + malignant_data)\n",
    "labels = np.array(benign_labels + malignant_labels)\n",
    "\n",
    "print(f\"Loaded {len(data)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bc5dc-e2e2-4746-a57c-e15f2e5cffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "data = data.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed0ead-8b6a-40bb-851f-ca9d5141367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Save the datasets as .npy files\n",
    "np.save(\"train_data.npy\", train_data)\n",
    "np.save(\"train_labels.npy\", train_labels)\n",
    "np.save(\"test_data.npy\", test_data)\n",
    "np.save(\"test_labels.npy\", test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec54e6a-05c3-404b-8f5d-1befbc13dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core import Run\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Get the Azure ML run context\n",
    "run = Run.get_context()\n",
    "\n",
    "def main():\n",
    "    # Define the file paths relative to the script\n",
    "    train_data_path = \"train_data.npy\"\n",
    "    train_labels_path = \"train_labels.npy\"\n",
    "    test_data_path = \"test_data.npy\"\n",
    "    test_labels_path = \"test_labels.npy\"\n",
    "\n",
    "    # Load the data\n",
    "    print(\"Loading data...\")\n",
    "    train_data = np.load(train_data_path)\n",
    "    train_labels = np.load(train_labels_path)\n",
    "    test_data = np.load(test_data_path)\n",
    "    test_labels = np.load(test_labels_path)\n",
    "    print(\"Data successfully loaded.\")\n",
    "\n",
    "    # Flatten the data for Random Forest\n",
    "    print(\"Preprocessing data...\")\n",
    "    train_data_flatten = train_data.reshape(len(train_data), -1)\n",
    "    test_data_flatten = test_data.reshape(len(test_data), -1)\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    print(\"Training the model...\")\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(train_data_flatten, train_labels)\n",
    "\n",
    "    # Predict on the test set\n",
    "    print(\"Evaluating the model...\")\n",
    "    test_predictions = rf_model.predict(test_data_flatten)\n",
    "\n",
    "    # Log metrics\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    run.log(\"accuracy\", accuracy)\n",
    "\n",
    "    # Log classification report\n",
    "    report = classification_report(test_labels, test_predictions, output_dict=True)\n",
    "    for label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            for metric_name, value in metrics.items():\n",
    "                run.log(f\"{label}_{metric_name}\", value)\n",
    "\n",
    "    # Save the model\n",
    "    print(\"Saving the model...\")\n",
    "    joblib.dump(rf_model, \"random_forest_model.pkl\")\n",
    "    print(\"Model saved as random_forest_model.pkl.\")\n",
    "\n",
    "    # Upload the model to Azure ML\n",
    "    print(\"Uploading the model to Azure ML...\")\n",
    "    run.upload_file(name=\"outputs/random_forest_model.pkl\", path_or_stream=\"random_forest_model.pkl\")\n",
    "    print(\"Model uploaded successfully.\")\n",
    "\n",
    "    # Save this script as a .py file\n",
    "    print(\"Saving the script as train.py...\")\n",
    "    script_content = \"\"\"\n",
    "import argparse\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core import Run\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Get the Azure ML run context\n",
    "run = Run.get_context()\n",
    "\n",
    "def main():\n",
    "    train_data_path = \"train_data.npy\"\n",
    "    train_labels_path = \"train_labels.npy\"\n",
    "    test_data_path = \"test_data.npy\"\n",
    "    test_labels_path = \"test_labels.npy\"\n",
    "\n",
    "    train_data = np.load(train_data_path)\n",
    "    train_labels = np.load(train_labels_path)\n",
    "    test_data = np.load(test_data_path)\n",
    "    test_labels = np.load(test_labels_path)\n",
    "\n",
    "    train_data_flatten = train_data.reshape(len(train_data), -1)\n",
    "    test_data_flatten = test_data.reshape(len(test_data), -1)\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(train_data_flatten, train_labels)\n",
    "\n",
    "    test_predictions = rf_model.predict(test_data_flatten)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    run.log(\"accuracy\", accuracy)\n",
    "\n",
    "    report = classification_report(test_labels, test_predictions, output_dict=True)\n",
    "    for label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            for metric_name, value in metrics.items():\n",
    "                run.log(f\"{label}_{metric_name}\", value)\n",
    "\n",
    "    joblib.dump(rf_model, \"random_forest_model.pkl\")\n",
    "    run.upload_file(name=\"outputs/random_forest_model.pkl\", path_or_stream=\"random_forest_model.pkl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "    with open(\"train.py\", \"w\") as script_file:\n",
    "        script_file.write(script_content)\n",
    "    run.upload_file(name=\"outputs/train.py\", path_or_stream=\"train.py\")\n",
    "    print(\"Script saved and uploaded successfully.\")\n",
    "\n",
    "# Entry point for the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71af2fdf-863f-4ac5-a6c1-a8fc70de623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = [\n",
    "    'train.py',\n",
    "    '--train_data', 'train_data.npy',\n",
    "    '--train_labels', 'train_labels.npy',\n",
    "    '--test_data', 'test_data.npy',\n",
    "    '--test_labels', 'test_labels.npy'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c174e93-8b55-42ac-ab55-f74e5d510bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core import Run\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Get the Azure ML run context\n",
    "run = Run.get_context()\n",
    "\n",
    "def main():\n",
    "    # Parse input arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data\", type=str, required=True)\n",
    "    parser.add_argument(\"--train_labels\", type=str, required=True)\n",
    "    parser.add_argument(\"--test_data\", type=str, required=True)\n",
    "    parser.add_argument(\"--test_labels\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load the data\n",
    "    print(\"Loading data from datastore...\")\n",
    "    train_data = np.load(args.train_data)\n",
    "    train_labels = np.load(args.train_labels)\n",
    "    test_data = np.load(args.test_data)\n",
    "    test_labels = np.load(args.test_labels)\n",
    "    print(\"Data successfully loaded.\")\n",
    "\n",
    "    # Flatten the data for Random Forest\n",
    "    print(\"Preprocessing data...\")\n",
    "    train_data_flatten = train_data.reshape(len(train_data), -1)\n",
    "    test_data_flatten = test_data.reshape(len(test_data), -1)\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    print(\"Training the model...\")\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(train_data_flatten, train_labels)\n",
    "\n",
    "    # Predict on the test set\n",
    "    print(\"Evaluating the model...\")\n",
    "    test_predictions = rf_model.predict(test_data_flatten)\n",
    "\n",
    "    # Log metrics\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    run.log(\"accuracy\", accuracy)\n",
    "\n",
    "    # Log classification report\n",
    "    report = classification_report(test_labels, test_predictions, output_dict=True)\n",
    "    for label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            for metric_name, value in metrics.items():\n",
    "                run.log(f\"{label}_{metric_name}\", value)\n",
    "\n",
    "    # Save the model\n",
    "    print(\"Saving the model...\")\n",
    "    joblib.dump(rf_model, \"random_forest_model.pkl\")\n",
    "    print(\"Model saved as random_forest_model.pkl.\")\n",
    "\n",
    "    # Upload the model to Azure ML\n",
    "    print(\"Uploading the model to Azure ML...\")\n",
    "    run.upload_file(name=\"outputs/random_forest_model.pkl\", path_or_stream=\"random_forest_model.pkl\")\n",
    "    print(\"Model uploaded successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2efa6-b2cb-4496-817e-3bd121ee270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "compute_name = \"naiks011\"  # Replace with your actual compute instance name\n",
    "\n",
    "# Retrieve the existing compute instance\n",
    "compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "print(f\"Using compute target: {compute_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55d228-3790-45cb-b454-35daf9c8ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "datastore = Datastore.get(ws, datastore_name=\"datalake_breastcancer\")\n",
    "datastore.upload_files(\n",
    "    files=[\"train_data.npy\", \"train_labels.npy\", \"test_data.npy\", \"test_labels.npy\"],\n",
    "    target_path=\"training_data\",\n",
    "    overwrite=True,\n",
    ")\n",
    "print(\"Files uploaded to datastore.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09197524-1430-431c-809e-9fc697c2b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install azureml-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047a121-0342-4bbd-ac03-7ea07110750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.data.datapath import DataPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfd4a6-0ef5-47de-ad75-8ffe0472d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = PythonScriptStep(\n",
    "    name=\"Train Random Forest Model\",\n",
    "    script_name=\"train.py\",\n",
    "    arguments=[\n",
    "        \"--train_data\", DataPath(datastore, \"training_data/train_data.npy\"),\n",
    "        \"--train_labels\", DataPath(datastore, \"training_data/train_labels.npy\"),\n",
    "        \"--test_data\", DataPath(datastore, \"training_data/test_data.npy\"),\n",
    "        \"--test_labels\", DataPath(datastore, \"training_data/test_labels.npy\"),\n",
    "    ],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=\"./pipeline_scripts\",  # Use the  directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672662f0-21b4-4641-98dd-ad42bd653564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and submit the pipeline\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[train_step])\n",
    "print(\"Pipeline created successfully.\")\n",
    "\n",
    "pipeline_run = pipeline.submit(\"breast-cancer-detection-pipeline\")\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ccbf6-b106-4cd3-bb78-dea394788789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
